@inproceedings{rajpurkar2018know,
    title="Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",
    author="Rajpurkar, Pranav and Jia, Robin and Liang, Percy",
    booktitle="Association for Computational Linguistics (ACL)",
    year="2018",
}
@inproceedings{Solberg2018ThePP,
  title={The predictive power of earnings conference calls : predicting stock price movement with earnings call transcripts},
  author={Lars Erik Solberg and J{\o}rgen Karlsen},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:56221989}
}
@article{roozen2021stock,
  title={Stock values and earnings call transcripts: a dataset suitable for sentiment analysis},
  author={Roozen, Dexter and Lelli, Francesco},
  year={2021},
  publisher={Preprints}
}
@article{cook2023evaluating,
  title={Evaluating Local Language Models: An Application to Financial Earnings Calls},
  author={Cook, Thomas R and Kazinnik, Sophia and Hansen, Anne Lundgaard and McAdam, Peter},
  journal={Available at SSRN 4627143},
  year={2023}
}
@phdthesis{brennan2021predicting,
  title={Predicting Stock Performance Using Quarterly Analyst’s Call Transcripts and Natural Language Processing (NLP): An Exploration},
  author={Brennan, Maureen},
  year={2021},
  school={Utica College}
}
@article{liang2016predicting,
  title={Predicting Stock Price Changes with Earnings Call Transcripts},
  author={Liang, Dong},
  year={2016}
}
@misc{ma2020earnings,
      title={Towards Earnings Call and Stock Price Movement}, 
      author={Zhiqiang Ma and Grace Bang and Chong Wang and Xiaomo Liu},
      year={2020},
      eprint={2009.01317},
      archivePrefix={arXiv},
      primaryClass={q-fin.ST}
}
@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}
@InProceedings{10.1007/3-540-59119-2_166,
author="Freund, Yoav
and Schapire, Robert E.",
editor="Vit{\'a}nyi, Paul",
title="A desicion-theoretic generalization of on-line learning and an application to boosting",
booktitle="Computational Learning Theory",
year="1995",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="23--37",
abstract="We consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update rule of Littlestone and Warmuth [10] can be adapted to this mode yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games and prediction of points in ân. We also show how the weight-update rule can be used to derive a new boosting algorithm which does not require prior knowledge about the performance of the weak learning algorithm.",
isbn="978-3-540-49195-8"
}
@article{4a848dd1-54e3-3c3c-83c3-04977ded2e71,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2699986},
 abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
 author = {Jerome H. Friedman},
 journal = {The Annals of Statistics},
 number = {5},
 pages = {1189--1232},
 publisher = {Institute of Mathematical Statistics},
 title = {Greedy Function Approximation: A Gradient Boosting Machine},
 urldate = {2024-03-16},
 volume = {29},
 year = {2001}
}


