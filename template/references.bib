@inproceedings{rajpurkar2018know,
    title="Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",
    author="Rajpurkar, Pranav and Jia, Robin and Liang, Percy",
    booktitle="Association for Computational Linguistics (ACL)",
    year="2018",
}
@inproceedings{Solberg2018ThePP,
  title={The predictive power of earnings conference calls : predicting stock price movement with earnings call transcripts},
  author={Lars Erik Solberg and J{\o}rgen Karlsen},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:56221989}
}
@article{roozen2021stock,
  title={Stock values and earnings call transcripts: a dataset suitable for sentiment analysis},
  author={Roozen, Dexter and Lelli, Francesco},
  year={2021},
  publisher={Preprints}
}
@article{cook2023evaluating,
  title={Evaluating Local Language Models: An Application to Financial Earnings Calls},
  author={Cook, Thomas R and Kazinnik, Sophia and Hansen, Anne Lundgaard and McAdam, Peter},
  journal={Available at SSRN 4627143},
  year={2023}
}
@phdthesis{brennan2021predicting,
  title={Predicting Stock Performance Using Quarterly Analyst’s Call Transcripts and Natural Language Processing (NLP): An Exploration},
  author={Brennan, Maureen},
  year={2021},
  school={Utica College}
}
@article{liang2016predicting,
  title={Predicting Stock Price Changes with Earnings Call Transcripts},
  author={Liang, Dong},
  year={2016}
}
@misc{ma2020earnings,
      title={Towards Earnings Call and Stock Price Movement}, 
      author={Zhiqiang Ma and Grace Bang and Chong Wang and Xiaomo Liu},
      year={2020},
      eprint={2009.01317},
      archivePrefix={arXiv},
      primaryClass={q-fin.ST}
}
@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}
@article{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}
@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}
@article{ma2020towards,
  title={Towards earnings call and stock price movement},
  author={Ma, Zhiqiang and Bang, Grace and Wang, Chong and Liu, Xiaomo},
  journal={arXiv preprint arXiv:2009.01317},
  year={2020}
}
@article{jayaraman2020can,
  title={Can Earnings Call Sentiment Predict Stock Price Movement?},
  author={Jayaraman, JD and Dennis, Andrew},
  journal={Proceedings of the Northeast Business \& Economics Association},
  year={2020}
}
@InProceedings{10.1007/3-540-59119-2_166,
author="Freund, Yoav
and Schapire, Robert E.",
editor="Vit{\'a}nyi, Paul",
title="A desicion-theoretic generalization of on-line learning and an application to boosting",
booktitle="Computational Learning Theory",
year="1995",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="23--37",
abstract="We consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update rule of Littlestone and Warmuth [10] can be adapted to this mode yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games and prediction of points in ân. We also show how the weight-update rule can be used to derive a new boosting algorithm which does not require prior knowledge about the performance of the weak learning algorithm.",
isbn="978-3-540-49195-8"
}
@article{4a848dd1-54e3-3c3c-83c3-04977ded2e71,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2699986},
 abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
 author = {Jerome H. Friedman},
 journal = {The Annals of Statistics},
 number = {5},
 pages = {1189--1232},
 publisher = {Institute of Mathematical Statistics},
 title = {Greedy Function Approximation: A Gradient Boosting Machine},
 urldate = {2024-03-16},
 volume = {29},
 year = {2001}
}
@article{JSSv045i03,
 title={mice: Multivariate Imputation by Chained Equations in R},
 volume={45},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v045i03},
 doi={10.18637/jss.v045.i03},
 abstract={The R package &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt; imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice, which extends the functionality of mice 1.0 in several ways. In &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt;, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt; adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt; can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
 number={3},
 journal={Journal of Statistical Software},
 author={van Buuren, Stef and Groothuis-Oudshoorn, Karin},
 year={2011},
 pages={167}
}
@misc{medya2022exploratory,
      title={An Exploratory Study of Stock Price Movements from Earnings Calls}, 
      author={Sourav Medya and Mohammad Rasoolinejad and Yang Yang and Brian Uzzi},
      year={2022},
      eprint={2203.12460},
      archivePrefix={arXiv},
      primaryClass={q-fin.ST}
}
@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}