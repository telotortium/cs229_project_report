\documentclass{article}

\usepackage[final]{neurips_2019}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{float} % for H specifier
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage{bm}



\newcommand{\note}[1]{\textcolor{blue}{{Note:~#1}}}
\newcommand{\TODO}[1]{\textcolor{red}{{TODO:~#1}}}
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Define a boolean variable
\newif\ifshowInstructions
% Uncomment the line below if you want the instructions to be shown by default, or comment it to hide instructions
%\showInstructionstrue

% Define the command
\newcommand{\instructions}[1]{%
    \ifshowInstructions%
        \textcolor{violet}{#1}%
    \fi%
}

\title{
  AI Tools for Earnings Call Transcript Analysis \\
  \vspace{1em}
  \small{\normalfont Stanford CS229 Project}  % Select one and delete the other
}

\author{
  Shawn Zhang  \\
  Department of Computer Science \\
  Stanford University \\
  \texttt{shawnz@stanford.edu} \\
  % Examples of more authors
   \And
  Robert Michael Irelan \\
  Department of Computer Science \\
  Stanford University \\
  \texttt{rirelan@stanford.edu} \\
   \And
Arman Akbarian Kaljahi  \\
  Department of Computer Science \\
  Stanford University \\
  \texttt{akbarian@stanford.edu} \\
}

\begin{document}

\maketitle

% \TODO{Add abstract back if space permits}
%\begin{abstract}
%The abstract is optional, depending on your available space. It should consist of 1 paragraph consisting of the motivation for your paper and a high-level explanation of the methodology you used/results obtained.
%\end{abstract}


% \instructions{This template does not contain the full instruction set for this assignment; please refer back to the milestone instructions PDF.}

\section{Introduction\instructions{{} | 0.5 page}}
\instructions{%
Explain the problem and why it is important. Discuss your motivation for pursuing this problem. Give some background if necessary. Clearly state what the input and output is. Be very explicit: “The input to our algorithm is an {image, amplitude, patient age, rainfall measurements, grayscale video, etc.}. We then use a {SVM, neural network, linear regression, etc.} to output a predicted {age, stock price, cancer type, music genre, etc.}.” This is very important since different teams have different inputs/outputs spanning different application domains. Being explicit about this makes it easier for readers. If you are using your project for multiple classes, add a paragraph explaining which components of the project were used for each class.}

Quarterly earnings call transcripts for publicly traded companies are 
often fairly large text documents that include lots of information about 
future growth of the company, its past performance and new initiatives 
and product launches. Once this information is released, if there is 
a strong and consistent positive outlook for the company in the 
content of the transcript, it results in a jump in stock price 
of the company in the next few days, while a negative sentiment 
and potential risks result in selloff of stock and price drop. 

In this project, we explore a two step ML technique where we
train a sentiment classifier model on large publicly available datasets 
and then use this classifier to extract sentiment statistics from the earning
call transcripts (a small dataset we created by crawling the web). 
In the second step, we combine these sentiment scores with other tabular and financial 
data we can extract from the earning call to train a smaller
classifier model that predicts stock price movement direction after earning call. 

In particular, the inputs used to construct the features of our algorithm consists of numbers detailed how much the stock's earnings per share (EPS) and revenues beat or missed Wall Street analyst expectations, as well as sentiment data derived from remarks by the CEO and CFO of the company during the earnings call.
The sentiment data is calculated using an NLP sentiment classifier (from scratch training of word embedding and averaging, or use LSTM to process vector sequence, as well as glove vectors and BERT pre-trained model).
We then use these features to classify each stock according to whether its price grew more or less than the growth of the overall market over the next 3 trading days after the earnings call.


\section{Related Work}
There are various related work in combining financial metrics from earning call with sentiment
and textual data to predict stock price movement. For example \citep{jayaraman2020can} 
and \citep{Solberg2018ThePP}
defines various "sentiment features" using lexical based approach (i.e. dictionary of words
and their sentiment values) and shows that it can equally contribute to prediction power in conjunction
with feature such as earning per share surprise. \citep{ma2020towards} uses a deep learning
framework and performs end to end prediction using a large attention based model. \citep{medya2022exploratory}
studies the effect of sentiment in conjunction with elements such as financial data (actual sale, 
estimated EPS) in particular with focus of how CEO's speech and emotional state impacts stock price
movement.  \citep{liang2016predicting} also combines lexical based sentiment with other
financial attributes for stock movement prediction. 

The lexical based approach is considered "classic" now with the developments of NLP now where
we have better semantic representation of words, so we decided to explore using more modern
sentiment analysis approach (deep learning and pre-trained models) than this. Similar to these
works we attempt to combine sentiment score of the transcript with other financial data for prediction. 
The approach in \citep{ma2020towards} is based on more modern NLP techniques such as attention mechanism
however it is end to end and requires large amount of data to extract both sentiment and financial attributes
we combined hand crafted feature extraction of financial attributes with deep learning based sentiment score.

\section{Dataset and Features}
\instructions{%
Describe your dataset: how many training/validation/test examples do you have? Is there any preprocessing you did? What about normalization or data augmentation? What is the resolution of your images? How is your time-series data discretized? Include a citation on where you obtained your dataset from. Depending on available space, show some examples from your dataset. You should also talk about the features you used. If you extracted features using Fourier transforms, word2vec, histogram of oriented gradients (HOG), PCA, ICA, etc. make sure to talk about it. Try to include examples of your data in the report (e.g. include an image, show a waveform, etc.).
}

The project's aim was to extract meaningful insights, particularly around CEO and CFO remarks to predict market reaction to these remarks. Here's a step-by-step breakdown of the process:
\begin{enumerate}
    \item Data Collection: The project commenced with the collection of earnings call transcripts. These documents were sourced from Seeking Alpha website (\hyperref[www]{https://seekingalpha.com/}).
  \item Pre-processing: The raw transcripts were converted from PDF to text files, facilitating easier manipulation and analysis. This conversion was crucial for the text-cleaning phase, as it enabled the removal of non-essential elements such as headers, footers, and URLs that could skew the analysis.
  \item Text Cleaning, Normalization and Feature Engineering: Advanced text-cleaning and normalization techniques were employed to refine the data further. This step involved converting all text to lowercase (or another uniform case), removing unnecessary punctuation, and resolving abbreviations to their full forms to standardize the text. Additionally, we eliminated greetings and common phrases at the beginning of speeches.
  \item Identifying and Extracting Key Remarks: One crucial step in our analysis involved pinpointing and extracting the remarks made by pivotal figures during the earnings calls, specifically the CEO (Chief Executive Officer) and the CFO (Chief Financial Officer). The process required devising a reliable method to accurately locate their names within the text and then isolate their subsequent remarks.

\end{enumerate}

To evaluate the reaction of stock price to earning calls, we need at minimum two types of raw data: earning call transcripts and historical stock price data.
We initially collected a large number of earning call transcripts, most of which were Q4 2023 earning reports from January-February 2024, as PDF printouts from the financial website Seeking Alpha.
These were separated into a training set of 684 transcripts and a test set of 163 transcripts.
%Both sets were then processed using a custom pipeline to extract the following features:
%\begin{itemize}
%  \item The stock symbol of the company
%  \item The date of the earning call
%  \item Stock prices at close for all trading days from 1 day before the earning call to 3 days after the earning call. These were downloaded from Yahoo Finance.
%  \item Whether the stock beat or missed analyst expectations for earnings per share (EPS) and revenue, and by how much
%  \item The names and titles (e.g., CEO, CFO, representative from an investment bank) for speakers on the call
%  \item Speaker for each paragraph in the earning call transcript
%\end{itemize}

In order to reduce the influence of broad market movements on the stock price reaction, we also downloaded from Yahoo Finance the price of the S\&P 500 index (using the SPY ETF).
We then calculated the excess return for each stock (i.e., the return of the stock relative to the market) as follows:
\begin{align*}
        %symbol_return = (symbol_frame.loc[symbol_max_date]["Close"] - symbol_frame.loc[symbol_min_date]["Close"]) / symbol_frame.loc[symbol_min_date]["Close"]
  \text{Return}(\text{stock}) &= \frac{\text{Price}_{t+3}(\text{stock})-\text{Price}_{t-1}(\text{stock})}{\text{Price}_{t-1}(\text{stock})} \\
  \text{ExcessReturn}(\text{stock}) &= \text{Return}(\text{stock}) - \text{Return}(\text{SPY})
\end{align*}

The primary label we predicted for each stock is whether the excess return of the stock was positive, negative or neutral.
%As an example, for the Q1 2024 earnings report, META had a postive excess return.
%This accords with the earnings call transcript, which includes
%the beating of EPS expectations and the positive leading comment from the CEO, both of which accord with the market-beating returns.

%\begin{minipage}[t]{0.45\textwidth}
%\begin{lstlisting}[caption={META: Stock Price Return Data (CSV)}, label=metareturncsv, frame=single, basicstyle=\small, numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt, showstringspaces=false, aboveskip=0pt, belowskip=0pt, captionpos=b, aboveskip=-0.5em, belowskip=-1.5em, xleftmargin=0.5em, xrightmargin=0.5em, breaklines=true, breakatwhitespace=true, escapeinside={(*@}{@*)}]
%Symbol,Return,Market Return,Risk-Free Return,Excess Return
%META,0.1518,0.0098,0.0004,0.1421
%\end{lstlisting}
%\end{minipage}\hfill
%\begin{minipage}[t]{0.45\textwidth}
%\begin{lstlisting}[caption={META: Structured Transcript with speakers (JSON)}, label=metajson, frame=single, basicstyle=\tiny, numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt, showstringspaces=false, aboveskip=0pt, belowskip=0pt, captionpos=b, aboveskip=-0.5em, belowskip=-1.5em, xleftmargin=0.5em, xrightmargin=0.5em, breaklines=true, breakatwhitespace=true, escapeinside={(*@}{@*)}]
%  "expectation_results": {
%    "eps": {
%      "value": "5.33",
%      "beats_by": "0.36"
%    },
%    "revenue": [...]
%  },
%  "participants": {
%    "Mark Zuckerberg": "Founder, Chairman & CEO",
%    [...]
%  },
%  "paragraphs": [
%    {
%      "speaker": "Mark Zuckerberg",
%      "text": "All right. Hi, everyone. Thanks for joining us. This was a good quarter, [...]"
%    },
%    [...]
%  ]
%\end{lstlisting}
%\end{minipage}

\section{Methods}
\instructions{%
Describe your learning algorithms, proposed algorithm(s), or theoretical proof(s). Make sure to include relevant mathematical notation. For example, you can briefly include the SVM optimization objective/formula or say what the softmax function is. It is okay to use formulas from the lecture notes. For each algorithm, give a short description (\~ 1 para- graph) of how it works. Again, we are looking for your understanding of how these machine learning algorithms work. Although the teaching staff probably know the algorithms, future readers may not (reports will be posted on the class website). Additionally, if you are using a niche or cutting-edge algorithm (e.g. long short-term memory, SURF features, or anything else not covered in the class), you may want to explain your algorithm using 1/2 paragraphs. Note: Theory/algorithms projects may have an appendix showing extended proofs (see Appendix section below).
}

\subsection{Sentiment Analysis and Text Classification}
We first explored the problem of text classification and machine learning methodologies used to represent text as a vector. Traditional 
approach to represent a text is bag of words (where each word
can be presented as a one-hot vector). The problem with this technique
is that every word is effectively orthogonal to another word in its
mathematical representation while we know there are semantic relations and 
similarities between words. We explored using dense representation
where we allow vectors to be trained as part of the neural network 
(starting at random). The better approach is to use pre-trained Word2Vec \citep{mikolov2013distributed} or Glove vectors \citep{pennington2014glove}, a large corpus of text
such as entire web crawl is used to learn a semantic vector to represent words.
In particular, the objective is to represents word $w$ with vector $\vec{w}$ such that the likelihood of observing word $w_i$ {\it near} (a window of 3 words to left and right for example) to word $w_j$ is given by:
\begin{equation}
p(w_i|w_j) = \frac{\exp(\vec{w}_i . \vec{w}_j)}{\sum_k \exp(\vec{w_i}.\vec{w_k})}
\end{equation}
Using the likelihood definition above and 
given a corpus and a definition of a window of {\it near} words, the
algorithm maximizes the likelihood of the observation of such co-occurance
of words in the corpus by tuning the word vectors.

For an ML model to learn sentiment of a sentence or paragraph, we explore
averaging word vectors of the paragraph as well as using LSTM {\bf cite LSTM}
from the category of recurrent neural networks. In short, in an RNN, 
the sequence of vectors, $\mathbf{x}^{(t)}$ (i.e. a sentence) are {\em read} 
from left to right with a nonlinear
transformations to update a hidden state $\mathbf{h^{(t)}}$ from
its previous state. In "vanilla" form, the forward propagation of
an RNN  can be written as:
\begin{equation}
    \mathbf{h^{(t)}} = f(\mathbf{h^{(t-1)}}, \mathbf{x}^{(t)}, \mathbf{W})
\end{equation}
where ${\mathbf W}$ encapsulates all the learnable parameters of the function $f$. The final state of the RNN, $\mathbf{h^{(T)}}$ (once full
sequence is read) is often used to represent the full sentence. 
In addition to LSTM, we also explore contextual representation
of words such as pre-trained BERT {\bf cite bert} model and
utilize IMDB dataset as a common dataset used for sentiment classification
benchmarking of various deep learning NLP architectures.

\subsection{Excess Return Prediction}
As described above, the primary label we are trying to predict for a given stock is the sign of the excess return.
This is a classification problem where the output is a binary value (positive or negative).
For this classification problem, we explored several standardized classification models, as implemented in Scikit-Learn \citep{scikit-learn}.
We decided to test three classifiers: logistic regression, AdaBoost \citep{10.1007/3-540-59119-2_166}, and Gradient Boosting \citep{4a848dd1-54e3-3c3c-83c3-04977ded2e71}.
More details of the hyperparameters, feature processing, and the like will be found in Section \ref{excess_return_prediction}.

\section{Experiments / Results / Discussion}
\instructions{%
You should also give details about what (hyper)parameters you chose (e.g. why did you use X learning rate for gradient descent, what was your mini-batch size and why) and how you chose them. Did you do cross-validation, if so, how many folds? Before you list your results, make sure to list and explain what your primary metrics are: accuracy, precision, AUC, etc. Provide equations for the metrics if necessary. For results, you want to have a mixture of tables and plots. If you are solving a classification problem, you should include a confusion matrix or AUC/AUPRC curves. Include performance metrics such as precision, recall, and accuracy. For regression problems, state the average error. You should have both quantitative and qualitative results. To reiterate, you must have both quantitative and qualitative results! This includes unsupervised learning (talk with your project TA on how to quantify unsupervised methods). Include visualizations of results, heatmaps, examples of where your algorithm failed and a discussion of why certain algorithms failed or succeeded. In addition, explain whether you think you have overfit to your training set and what, if anything, you did to mitigate that. Make sure to discuss the figures/tables in your main text throughout this section. Your plots should include legends, axis labels, and have font sizes that are legible when printed.
}
\subsection{Sentiment Classification}
For sentiment classification component, we experiment with random word vector initialization
of dimension 16, using dropout after embedding layer, trnucating input text to 512 words and 
we use averaging of all vectors and a dense/prediction output layer. This is denoted
as "Random WordVecs + Word Averaging" in the table below. We use 80-20 train and validation split
and standard IMDB test data for testing.

\begin{table}[b]
    \centering
    \begin{tabular}{c|c|c|c|c}
    \hline
        Model & Train Acc. & Valid Acc. & Test Acc. & Fin. Text Test Acc.  \\
        \hline
          Random WordVecs + Word Averaging & 0.971 & 0.891 & 0.888 & 0.614 \\
         \hline
          Random WordVecs + LSTM & 0.960 & 0.851 & 0.850 & 0.594 \\
         \hline
          Glove WordVecs + Word Averaging & 0.965 & 0.897 & 0.879 & 0.589 \\
         \hline
          Glove WordVecs + LSTM & 0.978 & 0.891 & 0.865 & 0.568 \\
         \hline
         BERT (trainable, pooling head) + Dense & 0.995 & {\bf 0.910} & {\bf 0.917} & {\bf 0.668} \\
         \hline
         
    \end{tabular}
    \caption{Comparison of various models, surprisingly our small dimensional random initialization
    of word vectors and training them on IMDB performed better than Glove vectors, however we did
    not define our vocabulary based on test data, which should be technically possible since
    these glove vectors already existing for test data as well as train. Our best model was
    the BERT model tuned and is the one we choose for next section.}
    \label{tab:my_label}
\end{table}

Our second model replaces the word averaging with an LSTM layer, here we tried various dimension
and our best model uses 64 embedding dimension, we truncated sequence length to 128, and the
dimension of the LSTM state is set to 64. This is denoted as "Random WordVecs + LSTM" in the table below.

In addition, we replace the random initilization with Glove vectors of dimension 300, and allow
them to be trainable, in this scenario we learned that large value of dropout (0.9) is
needed after embedding layer to prevent overfitting, and we added few dense layer before output layer.

Finally we use pre-trained BERT model and used the pooled layer output of it with an
addition of few dense layers and prediction layer. We allow the BERT weights to 
be trainable as well.

We also found a small financial dataset from Kaggle that has sentiment annotation, we wanted
to benchmark our model trained on IMDB on a different corpus of data that is more on
the domain of finance, to see potential out of distribution effect, as you can see 
from the table the model performace is significantly lower on this data indicating that
there is further benefit if we can find large corpus of financial datasets corpus with
sentiment annotation.



\subsection{Excess Return Prediction}
\label{excess_return_prediction}
\subsubsection{Hyperparameters}

For the logistic regression, we used L2 regularization.
In addition, since the revenue features are on the scale of millions to billions of dollars (both positive and negative), the EPS is on the order of tens to hundreds of dollars, and the sentiment scores are in the range $[0,1]$, it was necessary to scale each feature dimension to be on the same order of magnitude for logistic regression to work.
This was done using the \texttt{StandardScaler} class of Scikit-Learn, which transforms each feature to have zero mean and unit variance.
This transformation was not needed for the AdaBoost and Gradient Boosting classifier, so we did not perform it.

On the other hand, the performance of the AdaBoost and Gradient Boosting classifiers are sensitive both to the values of hyperparameters and to randomness during the optimization of the classifiers.
It was therefore necessary to perform a hyperparameter search over the most impactful hyperparameters: learning rate and number of classifiers (which apply to both AdaBoost and Gradient Boosting).
This was done using K-fold cross validation over the training set using randomized search.
We used $K=5$ to ensure that the vast majority of training data (80\%) remained to be used from training, and because $K=10$ showed worse performance.
Since both hyperparameters can vary over several orders of magnitude, both were drawn from a log-uniform distribution - learning rate from $[0.00001, 0.1]$ and number of classifiers from $(20, 1000)$.
Unlike with logistic regression, it was also necessary to optimize the search for balanced accuracy, given by \[\frac{1}{2}\left(\frac{TP}{TP+FN} + \frac{TN}{TN+FP}\right),\] in order to avoid the classifiers assigning almost all test data points to the negative class (i.e., return less than market return).

A limitation of our data analysis is that not all transcripts included EPS and revenue expectation results,
nor did they include comments by the CEO and CFO that could be used for our sentiment features.
About 20\% of our training data and 40\% of our test data was missing expectation results,
as well as a smaller amount of our training data.
In total, only slightly more than 50\% of the training data has all the features.
In order to still obtain some results when missing expectation or sentiment data, we have employed Scikit-Learn's \texttt{IterativeImputer} class, which uses the mice algorithm \citep{JSSv045i03}, to impute missing features.
For most of our results, we trained exclusively on training data containing all features, but trained an imputer on the training data as well, which we use to impute missing features in the test set for validation.
We did explore some training using the imputed features as well (see Table \ref{tab:excess_return_prediction_datasets}).

\subsubsection{Results}

The main classification metrics for our classifiers are located in Table \ref{tab:excess_return_prediction}.
We predict negative if the stock will fall relative to the market, and positive if it will rise.
As can be seen, the accuracy of all the methods is comparable.
Here, logistic regression is actually the most accurate method of all, despite being the simplest.
However, the confusion matrices and other metrics show a bias that all our methods have, towards predicting negative.
Logistic regression is accurate because it predicts 85\% of the data to be negative.
Because the test and validation datasets are both imbalanced, with $411/671 = 61.2\%$ of the training data actually negative and a similar percentage for the validation data, this bias improves the accuracy on the dataset.
For AdaBoost and Gradient Boosting, optimizing on accuracy would lead to almost all data classified negative.
This is why we optimize based on balanced accuracy.
For this reason, the accuracy for these methods is lower.


\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Metric} & \textbf{Logistic Regression} & \textbf{AdaBoost} & \textbf{Gradient Boosting} \\
\hline
Accuracy & $0.57$ & $0.53$ & $0.53$ \\
\hline
Negative Precision & $0.58$ & $0.59$ & $0.58$ \\
\hline
Negative Recall & $0.88$ & $0.57$ & $0.66$ \\
\hline
Positive Precision & $0.52$ & $0.47$ & $0.46$ \\
\hline
Positive Recall & $0.17$ & $0.49$ & $0.37$ \\
\hline
Confusion Matrix \\
\(
\begin{bmatrix}
TN & FP \\
FN & TP \\
\end{bmatrix}
\)
&
\(
\begin{bmatrix}
80 & 11 \\
58 & 12 \\
\end{bmatrix}
\)
&
\(
\begin{bmatrix}
52 & 39 \\
36 & 34 \\
\end{bmatrix}
\)
&
\(
\begin{bmatrix}
60 & 31 \\
44 & 26 \\
\end{bmatrix}
\)
\\
\end{tabular}
\caption{Results for stock movement (excess return) prediction\\
Predictions use all features (expectation and sentiment). Trained on training data for which all features are present. Predictions use imputed features where missing.}
\label{tab:excess_return_prediction}
\end{table}

To compare the importance of our features, we will compare the accuracy of Gradient Boosting, using the hyperparameters described above, on four sets of training data:
all features, using imputed features where needed (``imputed'');
expectation features only, with no imputation (``expectation'');
sentiment features only, with no imputation (``sentiment'');
and all features, with no imputation (``combined'').
All of these models ran against the test dataset with imputed features.
The result is shown in Table \ref{tab:excess_return_prediction_datasets}.
It can be seen that the greatest accuracy can be found when using only the expectation features.
In fact, adding features for the sentiment decreases accuracy somewhat.
On the other hand, the confusion matrices show that part of the superiority of the expectation-trained classifier is due to biasing heavily toward predicting negatives.
The imputed and combined classifiers show more balance.
While the sentiment-only classifier has the lowest accuracy, it can still predict better than chance, at $0.52$.
This demonstrates that even a sentiment-only classifier may have enough of an edge for a positive expected value trading strategy - in other words, it can be used to make money.

\begin{table}[b]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Accuracy} & \textbf{Confusion Matrix} \\
\hline
imputed & $0.54$
&
\(
\begin{bmatrix}
72 & 19 \\
55 & 15 \\
\end{bmatrix}
\) \\
\hline
expectation & $0.58$
&
\(
\begin{bmatrix}
90 & 1 \\
67 & 3 \\
\end{bmatrix}
\) \\
\hline
sentiment & $0.52$
&
\(
\begin{bmatrix}
78 & 13 \\
65 & 5 \\
\end{bmatrix}
\) \\
\hline
combined & $0.53$
&
\(
\begin{bmatrix}
60 & 31 \\
44 & 26
\end{bmatrix}
\) \\
\end{tabular}
\caption{Comparison between Gradient Boosting models trained on different features. Predictions are run on imputed data.}
\label{tab:excess_return_prediction_datasets}
\end{table}

\section{Conclusion / Future Work}
We have demonstrated and compared a variety of approaches for both extracting expectation and sentiment data from earnings call transcript and for predicting whether a stock will beat the market in the near term after an earnings call.
Our classifiers (logistic regression, AdaBoost, and Gradient Boosting) all showed comparable performance in predicting whether a stock would have excess return, no matter whether we used only expectation results, only sentiment analysis, or both.
While the accuracy of the prediction was not high, at between $0.5$ and $0.6$ on the test sets, it nevertheless was better than chance.

For sentiment analysis, one approach would be to follow the path shown in \citet{mikolov2013distributed}.

For classification, there are several foreseeable improvements to be made.
First, a broader set of features could be added, perhaps using more powerful LLMs such as Llama 2 \citep{touvron2023llama}.
A first step in this direction is \citet{cook2023evaluating}.
Moreover, we did not use any historical timeseries data (besides the minimal number of days needed to calculate the rturn).
This could help identify long-term trends, such as seasonality and sector performance, and also allow predicting longer timeseries after the earnings call.
Finally, rather than just predicting the sign, it is more useful to accurately predict whether a stock will greatly exceed or lag the market than to predict the sign of excess return for excess return close to 0, since larger stock movements are easier to transform into greater returns.


%\section{Appendices}
%\instructions{%
%This section is optional for theory/algorithmic projects. Include additional derivations of proofs which weren’t core to the understanding of your proposed algorithm in the methods section.
%Note: All sections before this point must fit on five (5) pages. No exceptions. Supplemental material is not allowed. Anything else you want to add to your report (e.g. acknowledgements, author bios, funding sources) is included in the 5 page limit. The exception is the section describing the contributions of each team member. You will be penalized -10 points per page exceeding this limit. The max report score is 100.
%}

\pagebreak

\section{Contributions}
Shawn established the first earning call transcript dataset for the project as well as drafting of the proposal doc. Robert wrote scripts to parse the earning call transcript dataset for a variety of information, such as speaker identification, expectations data, and earning call dates. He obtained stock price and S\&P500 data, and calculated the excess return which was used to make the labels for classification. He trained all the classifiers for excess return, using both the data mentioned before and the sentiment analysis data.
Arman established the notebook for text processing and input tokenization as well as tuning of an LSTM baseline model and replacing the embedding with glove embedding and tuning of BERT model and generating
statistics of sentiment score for CEO and CFO paragraphs. All team members equally contributed to finding various open source datasets for sentiment analysis and performing 
literature review and learning about various NLP and deep learning techniques.



%\section{Note on citations}
%\instructions{%
%There are two citation commands:
%}
%
%\instructions{%
%\paragraph{Citation in parentheses} The command \texttt{\textbackslash{}citep\{\}}  is for when you cite a paper at the end of a clause, and you could read the text out loud and not read the authors' names. For example ``We also run our experiments on a multilingual language model \citep{rajpurkar2018know}.'' 
%}
%
%\instructions{%
%\paragraph{Citation in text} The command \texttt{\textbackslash{}citet\{\}} is for in-text citations, when someone reading the text out loud would have to read the names of the authors for the sentence to make sense. For example, ``We also run our experiments on the multilingual model described in \citet{rajpurkar2018know}.''
%}

\bibliographystyle{acl_natbib}
\bibliography{references}


\end{document}
